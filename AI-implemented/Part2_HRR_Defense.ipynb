{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: HRR Defense Implementation\n",
    "\n",
    "## Holographically Reduced Representations for Privacy Protection\n",
    "\n",
    "This notebook implements the HRR defense mechanism from the paper:\n",
    "**\"Deploying CNNs on Untrusted Platforms Using 2D HRR\"** (arXiv:2206.05893)\n",
    "\n",
    "### Overview\n",
    "\n",
    "HRR uses 2D circular convolution in the frequency domain to \"bind\" inputs with secret keys, making the model's intermediate representations uninformative without the secret. This provides privacy against membership inference attacks.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Binding**: Obfuscate input by convolving with secret: $\\hat{x} = x \\circledast s$\n",
    "2. **Processing**: Server processes bound input: $r = f_W(\\hat{x})$\n",
    "3. **Unbinding**: User recovers information with secret: $y = f_P(r \\circledast s^\\dagger)$\n",
    "4. **Adversarial Training**: Ensures output $r$ is uninformative without secret\n",
    "\n",
    "### Mathematical Foundation:\n",
    "\n",
    "**Binding (Circular Convolution)**:\n",
    "$$x \\circledast s = \\mathcal{F}^{-1}[\\mathcal{F}(x) \\odot \\mathcal{F}(s)]$$\n",
    "\n",
    "Where:\n",
    "- $\\mathcal{F}$ = 2D Fourier Transform\n",
    "- $\\odot$ = element-wise multiplication\n",
    "- $s^\\dagger$ = inverse secret (complex conjugate / magnitude²)\n",
    "\n",
    "**Why FFT?**\n",
    "- Circular convolution in spatial domain = multiplication in frequency domain (Convolution Theorem)\n",
    "- Complexity: O(n log n) instead of O(n²)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np  # Numerical operations\n",
    "import torch  # PyTorch deep learning framework\n",
    "import torch.nn as nn  # Neural network modules\n",
    "import torch.optim as optim  # Optimization algorithms\n",
    "import torchvision  # Computer vision datasets and models\n",
    "import torchvision.transforms as transforms  # Data preprocessing\n",
    "from torch.utils.data import DataLoader  # Data loading utilities\n",
    "import torch.nn.functional as F  # Functional neural network operations\n",
    "from torchvision.models import resnet18  # ResNet-18 architecture\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "\n",
    "# Device configuration - use GPU if available for faster computation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HRR Operations\n",
    "\n",
    "### Secret Generation\n",
    "\n",
    "Generate a random secret key with special properties:\n",
    "1. Sample from normal distribution (Xavier-like initialization)\n",
    "2. Transform to frequency domain\n",
    "3. Project to unit magnitude\n",
    "4. Transform back to spatial domain\n",
    "\n",
    "**Why unit magnitude?**\n",
    "- Ensures numerical stability\n",
    "- Prevents gradient explosion/vanishing\n",
    "- Improves binding/unbinding quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_secret(H, W, C):\n",
    "    \"\"\"\n",
    "    Generate a secret key for HRR binding operation\n",
    "    \n",
    "    The secret is a random tensor with unit magnitude in frequency domain.\n",
    "    This ensures good binding properties and numerical stability.\n",
    "    \n",
    "    Process:\n",
    "    1. Sample from normal distribution (Xavier-like initialization)\n",
    "    2. Transform to frequency domain using 2D FFT\n",
    "    3. Project to unit magnitude (normalize)\n",
    "    4. Transform back to spatial domain\n",
    "    \n",
    "    Args:\n",
    "        H: Height of image (e.g., 32 for CIFAR-10)\n",
    "        W: Width of image (e.g., 32 for CIFAR-10)  \n",
    "        C: Number of channels (e.g., 3 for RGB)\n",
    "    \n",
    "    Returns:\n",
    "        Secret tensor of shape (C, H, W) - one secret per channel\n",
    "    \"\"\"\n",
    "    # Sample from normal distribution with variance scaling\n",
    "    # Factor 1/sqrt(H*W*C) helps maintain stable gradients\n",
    "    s = torch.randn(C, H, W, device=device) * (1.0 / np.sqrt(H * W * C))\n",
    "    \n",
    "    # Transform to frequency domain using 2D Fast Fourier Transform\n",
    "    # dim=(1, 2) applies FFT across height and width dimensions\n",
    "    F_s = torch.fft.fft2(s, dim=(1, 2))\n",
    "    \n",
    "    # Compute magnitude (absolute value) in frequency domain\n",
    "    magnitude = torch.abs(F_s)\n",
    "    \n",
    "    # Project to unit magnitude: divide by magnitude\n",
    "    # Add epsilon (1e-10) to avoid division by zero\n",
    "    # .real extracts real part after inverse FFT\n",
    "    s_projected = torch.fft.ifft2(F_s / (magnitude + 1e-10), dim=(1, 2)).real\n",
    "    \n",
    "    return s_projected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binding Operation\n",
    "\n",
    "Bind (obfuscate) image with secret using 2D circular convolution.\n",
    "\n",
    "**Convolution Theorem**:\n",
    "- Circular convolution in spatial domain = Element-wise multiplication in frequency domain\n",
    "- This is why we use FFT - it's much faster!\n",
    "\n",
    "**Security**:\n",
    "- Different secret for each sample/query\n",
    "- Without secret, bound image looks like random noise\n",
    "- Server cannot extract meaningful information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binding_2d(x, s):\n",
    "    \"\"\"\n",
    "    Bind (obfuscate) image x with secret s using 2D circular convolution\n",
    "    \n",
    "    This is the core HRR operation. In the frequency domain, circular\n",
    "    convolution becomes element-wise multiplication, making it efficient.\n",
    "    \n",
    "    Mathematical operation: x ⊛ s = F^(-1)[F(x) * F(s)]\n",
    "    where F is 2D FFT, * is element-wise multiplication\n",
    "    \n",
    "    Uses 2D FFT for efficient computation (O(n log n) instead of O(n²))\n",
    "    \n",
    "    Args:\n",
    "        x: Input image [C x H x W] - original data to protect\n",
    "        s: Secret vector [C x H x W] - encryption key\n",
    "    \n",
    "    Returns:\n",
    "        Bound (obfuscated) image [C x H x W] - can be sent to untrusted server\n",
    "    \"\"\"\n",
    "    # Step 1: Transform input image to frequency domain\n",
    "    # dim=(1, 2) means apply FFT across height and width, separately for each channel\n",
    "    F_x = torch.fft.fft2(x, dim=(1, 2))\n",
    "    \n",
    "    # Step 2: Transform secret to frequency domain\n",
    "    F_s = torch.fft.fft2(s, dim=(1, 2))\n",
    "    \n",
    "    # Step 3: Element-wise multiplication in frequency domain\n",
    "    # This implements circular convolution (⊛) in spatial domain\n",
    "    # Multiplication in frequency domain = convolution in spatial domain (convolution theorem)\n",
    "    B = F_x * F_s\n",
    "    \n",
    "    # Step 4: Inverse FFT to get result back in spatial domain\n",
    "    # .real extracts real part (imaginary part should be ~0 due to real inputs)\n",
    "    bound = torch.fft.ifft2(B, dim=(1, 2)).real\n",
    "    \n",
    "    return bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unbinding Operation\n",
    "\n",
    "Unbind (decrypt) bound image using the secret.\n",
    "\n",
    "**Key Point**: This only works with the CORRECT secret!\n",
    "- Wrong secret → garbage output\n",
    "- Correct secret → recovers meaningful representation\n",
    "\n",
    "**Inverse Secret**:\n",
    "- In frequency domain: $s^\\dagger = \\bar{s} / |s|^2$\n",
    "- $\\bar{s}$ = complex conjugate\n",
    "- $|s|^2$ = magnitude squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unbinding_2d(B, s):\n",
    "    \"\"\"\n",
    "    Unbind (decrypt) bound image B using the secret s\n",
    "    \n",
    "    This reverses the binding operation to recover the original image.\n",
    "    Only possible with the correct secret key!\n",
    "    \n",
    "    Mathematical operation: B ⊛ s† = F^(-1)[F(B) * F(s)^†]\n",
    "    where s† is the \"inverse\" of s (actually complex conjugate / magnitude²)\n",
    "    \n",
    "    Args:\n",
    "        B: Bound (obfuscated) image [C x H x W] - received from server\n",
    "        s: Secret vector [C x H x W] - decryption key (same as encryption key)\n",
    "    \n",
    "    Returns:\n",
    "        Unbound (decrypted) image [C x H x W] - approximate reconstruction of original\n",
    "    \"\"\"\n",
    "    # Step 1: Transform secret to frequency domain\n",
    "    F_s = torch.fft.fft2(s, dim=(1, 2))\n",
    "    \n",
    "    # Step 2: Compute inverse secret s†\n",
    "    # For complex numbers: inverse ≈ conjugate / (magnitude²)\n",
    "    # torch.conj() computes complex conjugate\n",
    "    # torch.abs(F_s)**2 computes magnitude squared\n",
    "    # Add epsilon to avoid division by zero\n",
    "    F_s_inv = torch.conj(F_s) / (torch.abs(F_s) ** 2 + 1e-10)\n",
    "    \n",
    "    # Step 3: Transform bound image to frequency domain\n",
    "    F_B = torch.fft.fft2(B, dim=(1, 2))\n",
    "    \n",
    "    # Step 4: Apply unbinding in frequency domain\n",
    "    # Element-wise multiplication with inverse secret\n",
    "    unbound = torch.fft.ifft2(F_B * F_s_inv, dim=(1, 2)).real\n",
    "    \n",
    "    return unbound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Network Architectures\n",
    "\n",
    "### Modified ResNet-18 (Main Network)\n",
    "\n",
    "**Critical Requirement**: Input and output must have SAME dimensions!\n",
    "\n",
    "Why?\n",
    "- Need to unbind output: requires same shape as bound input\n",
    "- Standard ResNet outputs class logits (small vector)\n",
    "- We need image-sized output (32x32x3 for CIFAR-10)\n",
    "\n",
    "**Architecture**:\n",
    "1. **Encoder**: ResNet-18 layers (downsampling)\n",
    "2. **Decoder**: Transposed convolutions (upsampling)\n",
    "3. **Output**: Same size as input (32x32x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedResNet18(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified ResNet-18 with encoder-decoder structure\n",
    "    \n",
    "    CRITICAL: Input and output have same dimensions (required for HRR)\n",
    "    \n",
    "    Architecture:\n",
    "    - Encoder: ResNet-18 backbone (conv1, layer1-4)\n",
    "    - Decoder: Transposed convolutions (upsample back to original size)\n",
    "    - Output: Same shape as input (e.g., 3x32x32 for CIFAR-10)\n",
    "    \n",
    "    This network runs on the UNTRUSTED server.\n",
    "    Input is bound, output is still obfuscated.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ModifiedResNet18, self).__init__()\n",
    "        \n",
    "        # Encoder: Use ResNet-18 backbone\n",
    "        resnet = resnet18(pretrained=False)\n",
    "        \n",
    "        # Extract convolutional layers (remove FC layers)\n",
    "        self.conv1 = resnet.conv1  # Initial conv: 64 channels\n",
    "        self.bn1 = resnet.bn1  # Batch normalization\n",
    "        self.relu = resnet.relu  # Activation function\n",
    "        self.maxpool = resnet.maxpool  # Downsampling\n",
    "        \n",
    "        # ResNet stages - progressively increase channels and decrease spatial size\n",
    "        self.layer1 = resnet.layer1  # 64 channels\n",
    "        self.layer2 = resnet.layer2  # 128 channels\n",
    "        self.layer3 = resnet.layer3  # 256 channels\n",
    "        self.layer4 = resnet.layer4  # 512 channels\n",
    "        \n",
    "        # Decoder: Upsample back to original dimensions\n",
    "        # Uses transposed convolutions (learnable upsampling)\n",
    "        self.decoder = nn.Sequential(\n",
    "            # From 512 to 256 channels, double spatial size\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # From 256 to 128 channels, double spatial size\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # From 128 to 64 channels, double spatial size\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # From 64 to 32 channels, double spatial size\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Final layer to match input channels (3 for RGB)\n",
    "            # Tanh activation: output in range [-1, 1]\n",
    "            nn.Conv2d(32, 3, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through encoder-decoder\n",
    "        \n",
    "        Input: Bound image (obfuscated)\n",
    "        Output: Processed image (still obfuscated)\n",
    "        \"\"\"\n",
    "        # Encoder: Downsample and extract features\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)  # Bottleneck: 512 channels, smallest spatial size\n",
    "        \n",
    "        # Decoder: Upsample back to input size\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x  # Same shape as input!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Network\n",
    "\n",
    "Takes unbound output and predicts class labels.\n",
    "\n",
    "**Where it runs**: User side (trusted)\n",
    "**Input**: Unbound representation (meaningful)\n",
    "**Output**: Class probabilities (10 classes for CIFAR-10)\n",
    "\n",
    "Standard CNN classifier architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Prediction network that takes unbound output and predicts class\n",
    "    \n",
    "    This network runs on the TRUSTED user side.\n",
    "    Input: Unbound representation (after applying secret)\n",
    "    Output: Class predictions (10 classes for CIFAR-10)\n",
    "    \n",
    "    Architecture: Standard CNN classifier\n",
    "    - 3 convolutional blocks with pooling\n",
    "    - 2 fully connected layers\n",
    "    - Dropout for regularization\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(PredictionNetwork, self).__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv block 1: 3 → 64 channels\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # Downsample: 32x32 → 16x16\n",
    "            \n",
    "            # Conv block 2: 64 → 128 channels\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # Downsample: 16x16 → 8x8\n",
    "            \n",
    "            # Conv block 3: 128 → 256 channels\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # Downsample: 8x8 → 4x4\n",
    "        )\n",
    "        \n",
    "        # Classification layers\n",
    "        # Input: 256 channels × 4×4 spatial = 4096 features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 4 * 4, 512),  # Hidden layer\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),  # Dropout for regularization (50% rate)\n",
    "            nn.Linear(512, num_classes)  # Output layer (10 classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Input: Unbound image [B x 3 x 32 x 32]\n",
    "        Output: Class logits [B x 10]\n",
    "        \"\"\"\n",
    "        x = self.features(x)  # Extract features: [B x 256 x 4 x 4]\n",
    "        x = x.view(x.size(0), -1)  # Flatten: [B x 4096]\n",
    "        x = self.classifier(x)  # Classify: [B x 10]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Reversal Layer\n",
    "\n",
    "**Key Innovation** for adversarial training!\n",
    "\n",
    "**Forward Pass**: Identity (pass data unchanged)\n",
    "**Backward Pass**: Negate gradients (flip sign)\n",
    "\n",
    "**Purpose**:\n",
    "- Force main network to produce uninformative outputs\n",
    "- Adversarial network tries to classify WITHOUT secret\n",
    "- Gradient reversal makes main network RESIST this\n",
    "- Result: Output $r$ is useless without secret $s$\n",
    "\n",
    "**Effect**:\n",
    "- Main network learns: \"Make output hard to classify\"\n",
    "- But still allows correct classification WITH secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReverseLayer(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Gradient Reversal Layer for adversarial training\n",
    "    \n",
    "    Forward: identity function (pass data through unchanged)\n",
    "    Backward: negates gradients (multiply by -1)\n",
    "    \n",
    "    This forces the main network to produce outputs that:\n",
    "    1. Work WITH secret (prediction network succeeds)\n",
    "    2. Don't work WITHOUT secret (adversarial network fails)\n",
    "    \n",
    "    The gradient reversal creates a minimax game:\n",
    "    - Main network tries to RESIST adversarial classification\n",
    "    - Adversarial network tries to classify anyway\n",
    "    - Equilibrium: output is uninformative without secret\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        \"\"\"\n",
    "        Forward pass: Identity function\n",
    "        \n",
    "        Simply returns input unchanged.\n",
    "        Context (ctx) stores info for backward pass.\n",
    "        \"\"\"\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass: Negate gradients\n",
    "        \n",
    "        Multiplies incoming gradients by -1.\n",
    "        This makes the network learn OPPOSITE of what adversary wants.\n",
    "        \"\"\"\n",
    "        return -grad_output  # Flip sign of gradients\n",
    "\n",
    "\n",
    "class AdversarialNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Adversarial network that tries to classify WITHOUT the secret\n",
    "    \n",
    "    Uses gradient reversal to force main network to be uninformative.\n",
    "    \n",
    "    Where it conceptually runs: Attacker (no secret)\n",
    "    Input: Raw output r from main network (no unbinding)\n",
    "    Output: Class predictions (should be random/bad)\n",
    "    \n",
    "    Same architecture as prediction network, but:\n",
    "    - Doesn't have access to secret\n",
    "    - Uses gradient reversal\n",
    "    - Should have LOW accuracy (indicates good privacy)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AdversarialNetwork, self).__init__()\n",
    "        \n",
    "        # Same architecture as prediction network\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 4 * 4, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass with gradient reversal\n",
    "        \n",
    "        The gradient reversal layer ensures that:\n",
    "        - When adversary tries to minimize classification loss\n",
    "        - Main network receives gradients to MAXIMIZE loss\n",
    "        - Result: Main network learns to be uninformative\n",
    "        \"\"\"\n",
    "        # Apply gradient reversal - this is where the magic happens!\n",
    "        x = GradientReverseLayer.apply(x)\n",
    "        \n",
    "        # Standard classification\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Function - CSPS Algorithm\n",
    "\n",
    "### Crypto-Oriented Neural Architecture Training\n",
    "\n",
    "**Three Networks**:\n",
    "1. **Main Network** ($f_W$): Processes bound inputs (server side)\n",
    "2. **Prediction Network** ($f_P$): Classifies unbound outputs (user side)\n",
    "3. **Adversarial Network** ($f_A$): Tries to classify without secret (attacker)\n",
    "\n",
    "**Training Loop**:\n",
    "```\n",
    "For each batch (x, y):\n",
    "    1. Generate NEW secret s (different each time!)\n",
    "    2. Bind: x̂ = x ⊛ s\n",
    "    3. Process: r = f_W(x̂)\n",
    "    4. Unbind: r̃ = r ⊛ s†\n",
    "    5. Predict: ŷ = f_P(r̃)\n",
    "    6. Attack: ŷ_adv = f_A(r)  [without secret!]\n",
    "    7. Loss: L = L_pred(ŷ, y) + L_adv(ŷ_adv, y)\n",
    "    8. Backprop with gradient reversal\n",
    "```\n",
    "\n",
    "**Key Points**:\n",
    "- New secret each sample prevents pattern analysis\n",
    "- Gradient reversal ensures privacy\n",
    "- Prediction accuracy should be GOOD\n",
    "- Adversarial accuracy should be BAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hrr_model(trainloader, epochs=30, use_adversarial=True):\n",
    "    \"\"\"\n",
    "    Train HRR-protected model using CSPS approach\n",
    "    \n",
    "    CSPS = Crypto-oriented Split Processing System\n",
    "    \n",
    "    Training involves three networks:\n",
    "    1. Main network (server): Processes bound inputs\n",
    "    2. Prediction network (user): Classifies with secret\n",
    "    3. Adversarial network (attacker): Tries without secret\n",
    "    \n",
    "    Args:\n",
    "        trainloader: DataLoader for training data\n",
    "        epochs: Number of training epochs\n",
    "        use_adversarial: Whether to use adversarial network (recommended: True)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (main_network, prediction_network, adversarial_network)\n",
    "    \"\"\"\n",
    "    # Initialize all three networks\n",
    "    main_network = ModifiedResNet18().to(device)\n",
    "    pred_network = PredictionNetwork().to(device)\n",
    "    adv_network = AdversarialNetwork().to(device) if use_adversarial else None\n",
    "    \n",
    "    # Separate optimizer for each network\n",
    "    optimizer_main = optim.Adam(main_network.parameters(), lr=0.001)\n",
    "    optimizer_pred = optim.Adam(pred_network.parameters(), lr=0.001)\n",
    "    optimizer_adv = optim.Adam(adv_network.parameters(), lr=0.001) if use_adversarial else None\n",
    "    \n",
    "    # Loss function: Cross-entropy for classification\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Set all networks to training mode\n",
    "        main_network.train()\n",
    "        pred_network.train()\n",
    "        if adv_network:\n",
    "            adv_network.train()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        correct_pred = 0  # Prediction network accuracy (should be high)\n",
    "        correct_adv = 0   # Adversarial network accuracy (should be low)\n",
    "        total = 0\n",
    "        \n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            \n",
    "            # CRITICAL: Generate NEW secrets for each sample in batch\n",
    "            # Different secret each time prevents pattern analysis\n",
    "            C, H, W = images.shape[1], images.shape[2], images.shape[3]\n",
    "            secrets = torch.stack([generate_secret(H, W, C) for _ in range(batch_size)])\n",
    "            \n",
    "            # Step 1: Bind inputs with secrets\n",
    "            # This happens on USER side before sending to server\n",
    "            bound_images = torch.stack([\n",
    "                binding_2d(images[j], secrets[j]) for j in range(batch_size)\n",
    "            ])\n",
    "            \n",
    "            # Step 2: Forward through main network\n",
    "            # This happens on UNTRUSTED SERVER\n",
    "            # Input is bound, output is still obfuscated\n",
    "            r = main_network(bound_images)\n",
    "            \n",
    "            # Step 3: Prediction network (USER side with secret)\n",
    "            # Unbind output using same secrets\n",
    "            unbound = torch.stack([\n",
    "                unbinding_2d(r[j], secrets[j]) for j in range(batch_size)\n",
    "            ])\n",
    "            pred_output = pred_network(unbound)\n",
    "            \n",
    "            # Calculate prediction loss (should be low = good accuracy)\n",
    "            loss_pred = criterion(pred_output, labels)\n",
    "            \n",
    "            # Step 4: Adversarial network (ATTACKER without secret)\n",
    "            # Tries to classify from raw output r\n",
    "            if adv_network:\n",
    "                adv_output = adv_network(r)  # No unbinding - doesn't have secret!\n",
    "                loss_adv = criterion(adv_output, labels)\n",
    "                # Total loss: prediction + adversarial\n",
    "                # Gradient reversal makes main network RESIST adversarial classification\n",
    "                total_loss = loss_pred + loss_adv\n",
    "            else:\n",
    "                total_loss = loss_pred\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer_main.zero_grad()\n",
    "            optimizer_pred.zero_grad()\n",
    "            if optimizer_adv:\n",
    "                optimizer_adv.zero_grad()\n",
    "            \n",
    "            # Backpropagation through all networks\n",
    "            # Gradient reversal affects how main network is updated\n",
    "            total_loss.backward()\n",
    "            \n",
    "            # Update all network parameters\n",
    "            optimizer_main.step()\n",
    "            optimizer_pred.step()\n",
    "            if optimizer_adv:\n",
    "                optimizer_adv.step()\n",
    "            \n",
    "            # Track statistics\n",
    "            running_loss += total_loss.item()\n",
    "            _, predicted = torch.max(pred_output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct_pred += (predicted == labels).sum().item()\n",
    "            \n",
    "            if adv_network:\n",
    "                _, predicted_adv = torch.max(adv_output.data, 1)\n",
    "                correct_adv += (predicted_adv == labels).sum().item()\n",
    "            \n",
    "            # Print progress every 50 batches\n",
    "            if (i + 1) % 50 == 0:\n",
    "                pred_acc = 100 * correct_pred / total\n",
    "                adv_acc = 100 * correct_adv / total if adv_network else 0\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(trainloader)}], '\n",
    "                      f'Loss: {running_loss/50:.4f}, Pred Acc: {pred_acc:.2f}%, '\n",
    "                      f'Adv Acc: {adv_acc:.2f}%')\n",
    "                running_loss = 0.0\n",
    "        \n",
    "        # Print epoch summary\n",
    "        epoch_pred_acc = 100 * correct_pred / total\n",
    "        epoch_adv_acc = 100 * correct_adv / total if adv_network else 0\n",
    "        print(f'Epoch [{epoch+1}/{epochs}] completed.')\n",
    "        print(f'  Prediction Accuracy: {epoch_pred_acc:.2f}% (should be HIGH)')\n",
    "        if adv_network:\n",
    "            print(f'  Adversarial Accuracy: {epoch_adv_acc:.2f}% (should be LOW for good privacy)')\n",
    "    \n",
    "    return main_network, pred_network, adv_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing Function\n",
    "\n",
    "Test the HRR-protected model on test set.\n",
    "\n",
    "**Same Pipeline as Training**:\n",
    "1. Generate secret\n",
    "2. Bind input\n",
    "3. Process through main network\n",
    "4. Unbind output\n",
    "5. Predict with prediction network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hrr_model(main_network, pred_network, testloader):\n",
    "    \"\"\"\n",
    "    Test HRR-protected model on test set\n",
    "    \n",
    "    Uses same pipeline as training:\n",
    "    1. Generate secret\n",
    "    2. Bind input\n",
    "    3. Process through main network\n",
    "    4. Unbind output\n",
    "    5. Classify with prediction network\n",
    "    \n",
    "    Args:\n",
    "        main_network: Trained main network\n",
    "        pred_network: Trained prediction network\n",
    "        testloader: DataLoader for test data\n",
    "    \n",
    "    Returns:\n",
    "        Test accuracy (percentage)\n",
    "    \"\"\"\n",
    "    # Set networks to evaluation mode\n",
    "    main_network.eval()\n",
    "    pred_network.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Disable gradient computation for inference\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            \n",
    "            # Generate secrets (new ones for test too)\n",
    "            C, H, W = images.shape[1], images.shape[2], images.shape[3]\n",
    "            secrets = torch.stack([generate_secret(H, W, C) for _ in range(batch_size)])\n",
    "            \n",
    "            # Bind inputs\n",
    "            bound_images = torch.stack([\n",
    "                binding_2d(images[j], secrets[j]) for j in range(batch_size)\n",
    "            ])\n",
    "            \n",
    "            # Process through main network\n",
    "            r = main_network(bound_images)\n",
    "            \n",
    "            # Unbind outputs\n",
    "            unbound = torch.stack([\n",
    "                unbinding_2d(r[j], secrets[j]) for j in range(batch_size)\n",
    "            ])\n",
    "            \n",
    "            # Predict\n",
    "            outputs = pred_network(unbound)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Execution - Train HRR-Protected and Baseline Models\n",
    "\n",
    "### Experiment Goals:\n",
    "\n",
    "1. Train HRR-protected model (3 networks)\n",
    "2. Train baseline model (standard ResNet-18)\n",
    "3. Compare test accuracies\n",
    "4. Expected trade-off: 5-10% accuracy loss for privacy\n",
    "\n",
    "### Configuration:\n",
    "- Training epochs: 30 (can increase for better results)\n",
    "- Batch size: 32 (smaller due to HRR overhead)\n",
    "- Use adversarial network: True (critical for privacy!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "TRAIN_EPOCHS = 30  # Can increase to 50-100 for better results\n",
    "BATCH_SIZE = 32    # Smaller batch size due to HRR computational overhead\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PART II: HRR DEFENSE IMPLEMENTATION\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "print(\"\\nLoading CIFAR-10 dataset...\")\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=True,\n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=False,\n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Optional: Use subset for faster training during development\n",
    "# Comment out these lines for full training\n",
    "train_subset_size = 10000\n",
    "trainset = torch.utils.data.Subset(trainset, range(train_subset_size))\n",
    "print(f\"Using subset of {train_subset_size} training samples for faster development\")\n",
    "\n",
    "# Create dataloaders\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Training samples: {len(trainset)}, Test samples: {len(testset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train HRR-Protected Model\n",
    "\n",
    "This will train all three networks together using the CSPS algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Training HRR-Protected Model...\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nThis will train:\")\n",
    "print(\"  1. Main Network (encoder-decoder on server)\")\n",
    "print(\"  2. Prediction Network (classifier on user side)\")\n",
    "print(\"  3. Adversarial Network (attacker without secret)\")\n",
    "print(\"\\nExpected: Prediction accuracy HIGH, Adversarial accuracy LOW\\n\")\n",
    "\n",
    "main_net, pred_net, adv_net = train_hrr_model(\n",
    "    trainloader, \n",
    "    epochs=TRAIN_EPOCHS,\n",
    "    use_adversarial=True  # CRITICAL for privacy!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test HRR-Protected Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Testing HRR-Protected Model...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_acc = test_hrr_model(main_net, pred_net, testloader)\n",
    "print(f\"\\nHRR-Protected Model Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "# Save models\n",
    "torch.save(main_net.state_dict(), 'hrr_main_network.pth')\n",
    "torch.save(pred_net.state_dict(), 'hrr_pred_network.pth')\n",
    "if adv_net:\n",
    "    torch.save(adv_net.state_dict(), 'hrr_adv_network.pth')\n",
    "\n",
    "print(\"\\nModels saved:\")\n",
    "print(\"  - hrr_main_network.pth\")\n",
    "print(\"  - hrr_pred_network.pth\")\n",
    "print(\"  - hrr_adv_network.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Baseline Model (No HRR)\n",
    "\n",
    "For comparison, train a standard ResNet-18 without HRR protection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Training Baseline Model (No HRR)...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize baseline model\n",
    "baseline_model = resnet18(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(baseline_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (standard training - no HRR)\n",
    "baseline_model.train()\n",
    "for epoch in range(TRAIN_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Standard training (no binding/unbinding)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = baseline_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{TRAIN_EPOCHS}], Step [{i+1}/{len(trainloader)}], '\n",
    "                  f'Loss: {running_loss/50:.4f}, Acc: {100*correct/total:.2f}%')\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    epoch_acc = 100 * correct / total\n",
    "    print(f'Epoch [{epoch+1}/{TRAIN_EPOCHS}] completed. Accuracy: {epoch_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test baseline model\n",
    "baseline_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = baseline_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "baseline_acc = 100 * correct / total\n",
    "print(f\"\\nBaseline Model Test Accuracy: {baseline_acc:.2f}%\")\n",
    "\n",
    "# Save baseline model\n",
    "torch.save(baseline_model.state_dict(), 'baseline_model.pth')\n",
    "print(\"Baseline model saved to 'baseline_model.pth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Summary and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART II COMPLETE - Summary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nHRR-Protected Model Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"Baseline Model Accuracy: {baseline_acc:.2f}%\")\n",
    "print(f\"Accuracy Drop: {baseline_acc - test_acc:.2f}%\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "if baseline_acc - test_acc < 5:\n",
    "    print(\"  ✓ Excellent! Minimal accuracy loss for privacy protection.\")\n",
    "elif baseline_acc - test_acc < 10:\n",
    "    print(\"  ✓ Good! Acceptable accuracy trade-off for privacy.\")\n",
    "else:\n",
    "    print(\"  ⚠ Significant accuracy loss. May need more training or tuning.\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Run evaluate_hrr_defense.py to test against RMIA attack\")\n",
    "print(\"  2. Compare AUC: baseline vs HRR-protected\")\n",
    "print(\"  3. Measure privacy-utility trade-off\")\n",
    "\n",
    "print(\"\\nAll models have been saved and ready for evaluation.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### What We Implemented:\n",
    "\n",
    "1. **HRR Operations**: Binding/unbinding using 2D FFT\n",
    "2. **Three Networks**: Main (server), Prediction (user), Adversarial (attacker)\n",
    "3. **CSPS Training**: Adversarial training with gradient reversal\n",
    "4. **Privacy Mechanism**: Output uninformative without secret\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **Trade-off**: Small accuracy loss (~5-10%) for privacy\n",
    "2. **Gradient Reversal**: Critical for forcing uninformative outputs\n",
    "3. **New Secrets**: Different key per sample prevents pattern analysis\n",
    "4. **FFT Efficiency**: Makes HRR practical (O(n log n) complexity)\n",
    "\n",
    "### TASK 2.2 Questions Preview:\n",
    "\n",
    "**Q1: How effective is HRR at preventing RMIA?**\n",
    "- Will measure AUC reduction in next notebook\n",
    "- Expected: 20-30% AUC drop (attack becomes much harder)\n",
    "\n",
    "**Q2: Does HRR qualify as encryption?**\n",
    "- No - it's obfuscation, not cryptographic encryption\n",
    "- Provides practical privacy, not provable security\n",
    "- Good for cost-effective protection\n",
    "\n",
    "**Q3: Could attackers adapt?**\n",
    "- Clustering attacks: Fail (ARI < 2%)\n",
    "- Inversion attacks: Fail (poor reconstruction)\n",
    "- Supervised learning: Limited success (2-5× random)\n",
    "- Gradient reversal makes adaptation very hard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
