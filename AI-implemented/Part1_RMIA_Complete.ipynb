{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Complete RMIA Implementation\n",
    "\n",
    "## Robust Membership Inference Attack against ResNet-18 on CIFAR-10\n",
    "\n",
    "This notebook implements the RMIA attack from the paper:\n",
    "**\"Membership Inference Attacks From First Principles\"** (arXiv:2112.03570)\n",
    "\n",
    "### Overview\n",
    "\n",
    "The attack determines if a specific data sample was used in training a model by comparing likelihood ratios between the target model and reference models.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Likelihood Ratio**: Measures how much more likely the target model assigns to a sample vs reference models\n",
    "2. **Pairwise Comparison**: Compares test sample against population samples\n",
    "3. **Reference Models**: Trained on different data to estimate population distribution\n",
    "4. **RMIA Score**: Fraction of population samples that test sample \"dominates\"\n",
    "\n",
    "### Mathematical Foundation:\n",
    "\n",
    "$$LR_\\theta(x, z) = \\frac{Pr(x|\\theta) / Pr(x)}{Pr(z|\\theta) / Pr(z)}$$\n",
    "\n",
    "Where:\n",
    "- $Pr(x|\\theta)$ = probability assigned by target model\n",
    "- $Pr(x)$ = estimated population probability (from reference models)\n",
    "- $x$ = test sample, $z$ = population sample\n",
    "\n",
    "**RMIA Score**: $Score(x) = Pr_z[LR_\\theta(x,z) \\geq \\gamma]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np  # For numerical operations\n",
    "import torch  # PyTorch framework\n",
    "import torch.nn as nn  # Neural network modules\n",
    "import torch.optim as optim  # Optimization algorithms\n",
    "import torchvision  # Computer vision datasets and models\n",
    "import torchvision.transforms as transforms  # Data preprocessing\n",
    "from torchvision.models import resnet18  # ResNet-18 architecture\n",
    "from torch.utils.data import DataLoader, Subset  # Data loading utilities\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score  # Evaluation metrics\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "import pickle  # For saving Python objects\n",
    "import os  # Operating system interface\n",
    "\n",
    "# Device configuration - use GPU if available for faster training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training Function\n",
    "\n",
    "This function trains a ResNet-18 model on CIFAR-10 dataset.\n",
    "\n",
    "### Training Process:\n",
    "1. **Forward Pass**: Input → Model → Predictions\n",
    "2. **Loss Calculation**: Compare predictions with true labels\n",
    "3. **Backward Pass**: Compute gradients via backpropagation\n",
    "4. **Optimization Step**: Update model weights\n",
    "\n",
    "### Why ResNet-18?\n",
    "- Good balance between performance and computational cost\n",
    "- Deep enough to memorize training data (important for MIA)\n",
    "- Standard architecture for CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloader, epochs=10, model_name=\"model\"):\n",
    "    \"\"\"\n",
    "    Train a ResNet-18 model on CIFAR-10\n",
    "    \n",
    "    Args:\n",
    "        dataloader: PyTorch DataLoader containing training data\n",
    "        epochs: Number of complete passes through the dataset\n",
    "        model_name: Name for logging purposes\n",
    "    \n",
    "    Returns:\n",
    "        Trained PyTorch model\n",
    "    \"\"\"\n",
    "    # Initialize ResNet-18 with 10 output classes (for CIFAR-10)\n",
    "    model = resnet18(num_classes=10).to(device)\n",
    "    \n",
    "    # Cross-entropy loss - standard for classification tasks\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Adam optimizer - adaptive learning rate optimization\n",
    "    # lr=0.001 is a common default learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Set model to training mode (enables dropout, batch norm updates, etc.)\n",
    "    model.train()\n",
    "    \n",
    "    # Loop over the dataset multiple times\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0  # Track cumulative loss\n",
    "        correct = 0  # Count correct predictions\n",
    "        total = 0  # Count total samples processed\n",
    "        \n",
    "        # Iterate through batches of data\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            # Move data to GPU/CPU\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients (PyTorch accumulates gradients by default)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass: compute model predictions\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Calculate loss between predictions and true labels\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass: compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update model parameters based on gradients\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics tracking\n",
    "            running_loss += loss.item()  # Accumulate loss\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get predicted class (highest probability)\n",
    "            total += labels.size(0)  # Count samples in this batch\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "            \n",
    "            # Print progress every 100 batches\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'{model_name} - Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(dataloader)}], '\n",
    "                      f'Loss: {running_loss/100:.4f}, Acc: {100*correct/total:.2f}%')\n",
    "                running_loss = 0.0  # Reset running loss for next 100 batches\n",
    "        \n",
    "        # Calculate and print accuracy for the entire epoch\n",
    "        epoch_acc = 100 * correct / total\n",
    "        print(f'{model_name} - Epoch [{epoch+1}/{epochs}] completed. Accuracy: {epoch_acc:.2f}%')\n",
    "    \n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RMIA Score Calculation with Multiple Reference Models\n",
    "\n",
    "### The RMIA Algorithm:\n",
    "\n",
    "For each test sample $x$:\n",
    "\n",
    "1. **Compute target model probability**: $Pr(x|\\theta_{target})$\n",
    "2. **Estimate population probability**: Average over reference models\n",
    "   $$Pr(x)_{OUT} = \\frac{1}{K}\\sum_{i=1}^K Pr(x|\\theta_{ref}^i)$$\n",
    "3. **Apply offline scaling**: Interpolate with uniform distribution\n",
    "   $$Pr(x) = 0.5[(1+a)Pr(x)_{OUT} + (1-a)]$$\n",
    "4. **Compute likelihood ratio**: \n",
    "   $$ratio(x) = \\frac{Pr(x|\\theta_{target})}{Pr(x)}$$\n",
    "5. **Compare against population**: Count how many samples $z$ satisfy:\n",
    "   $$\\frac{ratio(x)}{ratio(z)} \\geq \\gamma$$\n",
    "6. **RMIA Score**: Fraction of dominated samples (0 to 1)\n",
    "\n",
    "### Parameters:\n",
    "- **gamma (γ)**: Threshold for domination (default: 1.0)\n",
    "- **a**: Offline scaling parameter (default: 0.3)\n",
    "- Higher score → more likely a member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmia_score_multi(target_model, ref_models, known_img, known_label, \n",
    "                         population_subset, gamma=1.0, a=0.3):\n",
    "    \"\"\"\n",
    "    Calculate RMIA score using multiple reference models\n",
    "    \n",
    "    RMIA Concept:\n",
    "    - Compares how much more likely the target model assigns to a sample vs reference models\n",
    "    - Uses pairwise likelihood ratios: LR(x,z) = [Pr(x|θ) / Pr(x)] / [Pr(z|θ) / Pr(z)]\n",
    "    - Score = fraction of population samples that x \"dominates\"\n",
    "    \n",
    "    Args:\n",
    "        target_model: The model being attacked (trained on members)\n",
    "        ref_models: List of reference models (trained on different data)\n",
    "        known_img: Image to test for membership\n",
    "        known_label: True label of the image\n",
    "        population_subset: List of (img, label) tuples used as baseline comparison\n",
    "        gamma: Threshold for determining if x dominates z (default: 1.0)\n",
    "        a: Offline scaling parameter to approximate Pr(x) (default: 0.3)\n",
    "    \n",
    "    Returns:\n",
    "        RMIA score between 0 and 1 (higher = more likely a member)\n",
    "    \"\"\"\n",
    "    # Set all models to evaluation mode (disables dropout, batch norm uses running stats)\n",
    "    target_model.eval()\n",
    "    for rm in ref_models:\n",
    "        rm.eval()\n",
    "    \n",
    "    # Disable gradient computation (saves memory and speeds up inference)\n",
    "    with torch.no_grad():\n",
    "        # Move image to GPU/CPU\n",
    "        known_img = known_img.to(device)\n",
    "        \n",
    "        # Step 1: Get probability that target model assigns to the correct class\n",
    "        # unsqueeze(0) adds batch dimension, softmax converts logits to probabilities\n",
    "        prob_x_target = torch.softmax(target_model(known_img.unsqueeze(0)), dim=1)[0, known_label].item()\n",
    "        \n",
    "        # Step 2: Average predictions across all reference models to estimate Pr(x)_OUT\n",
    "        # This approximates the probability distribution of models NOT trained on x\n",
    "        all_ref_probs_x = []\n",
    "        for rm in ref_models:\n",
    "            prob = torch.softmax(rm(known_img.unsqueeze(0)), dim=1)[0, known_label].item()\n",
    "            all_ref_probs_x.append(prob)\n",
    "        prob_x_out = np.mean(all_ref_probs_x)  # Average over all reference models\n",
    "        \n",
    "        # Step 3: Offline scaling approximation (from RMIA paper Equation 5)\n",
    "        # Interpolates between OUT probability and uniform distribution\n",
    "        # a=0 gives uniform, a=1 gives pure OUT estimate\n",
    "        pr_x = 0.5 * ((1 + a) * prob_x_out + (1 - a))\n",
    "        # Add epsilon (1e-10) to avoid division by zero\n",
    "        ratio_x = prob_x_target / (pr_x + 1e-10)\n",
    "        \n",
    "        # Step 4: Count how many population samples x \"dominates\"\n",
    "        # x dominates z if LR(x,z) >= gamma\n",
    "        count_dominated = 0\n",
    "        for z_img, z_label in population_subset:\n",
    "            z_img = z_img.to(device)\n",
    "            \n",
    "            # Get target model probability for population sample z\n",
    "            prob_z_target = torch.softmax(target_model(z_img.unsqueeze(0)), dim=1)[0, z_label].item()\n",
    "            \n",
    "            # Average reference model predictions for z\n",
    "            all_ref_probs_z = []\n",
    "            for rm in ref_models:\n",
    "                prob = torch.softmax(rm(z_img.unsqueeze(0)), dim=1)[0, z_label].item()\n",
    "                all_ref_probs_z.append(prob)\n",
    "            prob_z_out = np.mean(all_ref_probs_z)\n",
    "            \n",
    "            # Apply same offline scaling to z\n",
    "            pr_z = 0.5 * ((1 + a) * prob_z_out + (1 - a))\n",
    "            ratio_z = prob_z_target / (pr_z + 1e-10)\n",
    "            \n",
    "            # Check if x dominates z (likelihood ratio comparison)\n",
    "            # If true, x is more \"member-like\" than z\n",
    "            if (ratio_x / (ratio_z + 1e-10)) >= gamma:\n",
    "                count_dominated += 1\n",
    "        \n",
    "        # Return score: proportion of population dominated by x\n",
    "        # Score close to 1 = likely member, close to 0 = likely non-member\n",
    "        return count_dominated / len(population_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Attack Evaluation Function\n",
    "\n",
    "### Evaluation Methodology:\n",
    "\n",
    "1. **Sample Selection**: Test on both members and non-members\n",
    "2. **Score Calculation**: Compute RMIA score for each sample\n",
    "3. **ROC Curve**: Plot True Positive Rate vs False Positive Rate\n",
    "4. **AUC Metric**: Area Under ROC Curve (0.5 = random, 1.0 = perfect)\n",
    "\n",
    "### Key Metrics:\n",
    "- **AUC**: Overall attack effectiveness\n",
    "- **TPR at low FPR**: Precision in identifying members\n",
    "- **Score Distribution**: Separation between members and non-members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_attack(target_model, ref_models, members, non_members, \n",
    "                    population_data, num_eval=500, population_size=1000):\n",
    "    \"\"\"\n",
    "    Evaluate RMIA attack performance on members and non-members\n",
    "    \n",
    "    Computes ROC curve and AUC to measure attack effectiveness\n",
    "    \n",
    "    Args:\n",
    "        target_model: Model being attacked\n",
    "        ref_models: List of reference models\n",
    "        members: Dataset of training samples (members)\n",
    "        non_members: Dataset of non-training samples (non-members)\n",
    "        population_data: Population dataset for baseline\n",
    "        num_eval: Number of samples to evaluate from each set\n",
    "        population_size: Size of population subset to use\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with scores, labels, fpr, tpr, and auc\n",
    "    \"\"\"\n",
    "    print(f\"\\nEvaluating attack with {len(ref_models)} reference models...\")\n",
    "    print(f\"Testing on {num_eval} members and {num_eval} non-members\")\n",
    "    \n",
    "    # Lists to store all scores and corresponding labels\n",
    "    all_scores = []  # RMIA scores for each sample\n",
    "    all_labels = []  # 1 for members, 0 for non-members\n",
    "    \n",
    "    # Sample a subset of population data for baseline comparison in RMIA\n",
    "    # This serves as the \"z\" samples in the likelihood ratio comparisons\n",
    "    population_subset = [population_data[i] for i in range(min(population_size, len(population_data)))]\n",
    "    \n",
    "    # Evaluate attack on member samples (should get high scores)\n",
    "    print(\"Testing members...\")\n",
    "    for i in range(min(num_eval, len(members))):\n",
    "        img, label = members[i]  # Get image and its true label\n",
    "        # Calculate RMIA score for this member\n",
    "        score = get_rmia_score_multi(target_model, ref_models, img, label, population_subset)\n",
    "        all_scores.append(score)\n",
    "        all_labels.append(1)  # Label 1 indicates this is a member\n",
    "        \n",
    "        # Print progress every 100 samples\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  Processed {i+1}/{num_eval} members\")\n",
    "    \n",
    "    # Evaluate attack on non-member samples (should get low scores)\n",
    "    print(\"Testing non-members...\")\n",
    "    for i in range(min(num_eval, len(non_members))):\n",
    "        img, label = non_members[i]  # Get image and its true label\n",
    "        # Calculate RMIA score for this non-member\n",
    "        score = get_rmia_score_multi(target_model, ref_models, img, label, population_subset)\n",
    "        all_scores.append(score)\n",
    "        all_labels.append(0)  # Label 0 indicates this is a non-member\n",
    "        \n",
    "        # Print progress every 100 samples\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  Processed {i+1}/{num_eval} non-members\")\n",
    "    \n",
    "    # Calculate ROC curve: plots True Positive Rate vs False Positive Rate\n",
    "    # at different threshold values for the RMIA score\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_scores)\n",
    "    \n",
    "    # Calculate AUC (Area Under Curve) - single number measuring attack effectiveness\n",
    "    # AUC = 0.5 means random guessing, AUC = 1.0 means perfect attack\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  AUC: {roc_auc:.4f}\")\n",
    "    # TPR at low FPR is important: shows how many members we catch with few false alarms\n",
    "    print(f\"  TPR at 1% FPR: {tpr[np.where(fpr <= 0.01)[0][-1]] if np.any(fpr <= 0.01) else 0:.4f}\")\n",
    "    print(f\"  TPR at 0.1% FPR: {tpr[np.where(fpr <= 0.001)[0][-1]] if np.any(fpr <= 0.001) else 0:.4f}\")\n",
    "    \n",
    "    # Return all results in a dictionary for later analysis\n",
    "    return {\n",
    "        'scores': all_scores,  # List of RMIA scores\n",
    "        'labels': all_labels,  # List of ground truth labels\n",
    "        'fpr': fpr,  # False positive rates for ROC curve\n",
    "        'tpr': tpr,  # True positive rates for ROC curve\n",
    "        'auc': roc_auc,  # Area under ROC curve\n",
    "        'thresholds': thresholds  # Score thresholds corresponding to FPR/TPR\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization Functions\n",
    "\n",
    "### ROC Curve:\n",
    "- X-axis: False Positive Rate (non-members classified as members)\n",
    "- Y-axis: True Positive Rate (members correctly identified)\n",
    "- Diagonal line: Random guessing (AUC = 0.5)\n",
    "- Closer to top-left corner = better attack\n",
    "\n",
    "### Score Distribution:\n",
    "- Histogram showing score frequencies\n",
    "- Good separation = effective attack\n",
    "- Overlap = harder to distinguish members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves(results_dict, save_path='roc_comparison.png'):\n",
    "    \"\"\"\n",
    "    Plot ROC curves for different attack configurations\n",
    "    \n",
    "    ROC (Receiver Operating Characteristic) curve shows the trade-off between\n",
    "    True Positive Rate (correctly identified members) and False Positive Rate\n",
    "    (non-members incorrectly identified as members) at different thresholds\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dictionary mapping configuration names to result dictionaries\n",
    "        save_path: Filename to save the plot\n",
    "    \"\"\"\n",
    "    # Create a large figure for better visibility\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot one ROC curve for each configuration\n",
    "    for name, results in results_dict.items():\n",
    "        plt.plot(results['fpr'], results['tpr'], \n",
    "                label=f'{name} (AUC = {results[\"auc\"]:.4f})')\n",
    "    \n",
    "    # Plot diagonal line representing random guessing (AUC = 0.5)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "    \n",
    "    # Set axis limits\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    \n",
    "    # Add labels and formatting\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves - RMIA Attack Performance')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Save to file\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"ROC curve saved to {save_path}\")\n",
    "\n",
    "\n",
    "def plot_score_distribution(results, save_path='score_distribution.png'):\n",
    "    \"\"\"\n",
    "    Plot histogram of RMIA scores for members vs non-members\n",
    "    \n",
    "    Good separation between distributions indicates effective attack\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary containing scores and labels\n",
    "        save_path: Filename to save the plot\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    scores = np.array(results['scores'])\n",
    "    labels = np.array(results['labels'])\n",
    "    \n",
    "    # Separate scores by membership status\n",
    "    member_scores = scores[labels == 1]  # Scores for training samples\n",
    "    non_member_scores = scores[labels == 0]  # Scores for non-training samples\n",
    "    \n",
    "    # Create histogram plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot overlapping histograms with transparency\n",
    "    # density=True normalizes to show probability density\n",
    "    plt.hist(member_scores, bins=30, alpha=0.6, color='blue', label='Members', density=True)\n",
    "    plt.hist(non_member_scores, bins=30, alpha=0.6, color='orange', label='Non-Members', density=True)\n",
    "    \n",
    "    # Add labels and formatting\n",
    "    plt.xlabel('RMIA Score')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Distribution of Membership Scores')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Save to file\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Score distribution saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Execution - RMIA Attack with Multiple Reference Models\n",
    "\n",
    "### Experiment Setup:\n",
    "\n",
    "1. **Data Split**:\n",
    "   - 20,000 members (used to train target model)\n",
    "   - 20,000 non-members (never seen by target model)\n",
    "   - 10,000 population (used to train reference models)\n",
    "\n",
    "2. **Configurations**:\n",
    "   - Test with 1, 2, 4, 8 reference models\n",
    "   - More reference models = more stable Pr(x) estimate\n",
    "   - Diminishing returns after 4-8 models\n",
    "\n",
    "3. **Expected Results** (from paper):\n",
    "   - 1 ref model: AUC ~68-69%\n",
    "   - 2 ref models: AUC ~70-71%\n",
    "   - 4 ref models: AUC ~71-72%\n",
    "   - 8 ref models: AUC ~71-73%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "TRAIN_EPOCHS = 10  # Increase to 50-100 for better results (closer to paper)\n",
    "NUM_REF_MODELS = [1, 2, 4, 8]  # Test with different numbers of reference models\n",
    "NUM_EVAL_SAMPLES = 500  # Number of samples to evaluate (increase for more reliable results)\n",
    "POPULATION_SIZE = 1000  # Size of population for likelihood ratio comparisons\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PART I: ROBUST MEMBERSHIP INFERENCE ATTACK (RMIA)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing: normalize to [-1, 1] range\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL Image to tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize each channel\n",
    "])\n",
    "\n",
    "print(\"\\nLoading CIFAR-10 dataset...\")\n",
    "full_trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    download=True,  # Downloads if not already present\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Split dataset into three disjoint sets\n",
    "# This ensures no data leakage between target model, reference models, and evaluation\n",
    "target_train, target_test, population_data, _ = torch.utils.data.random_split(\n",
    "    full_trainset, \n",
    "    [20000, 20000, 10000, 0]  # Members, Non-members, Population, Remainder\n",
    ")\n",
    "\n",
    "# Create dataloader for target model training\n",
    "trainloader = DataLoader(\n",
    "    target_train, \n",
    "    batch_size=64,  # Process 64 images at a time\n",
    "    shuffle=True,  # Randomize order for better training\n",
    "    num_workers=2  # Use 2 processes for data loading (speeds up training)\n",
    ")\n",
    "\n",
    "print(f\"Dataset split: {len(target_train)} members, {len(target_test)} non-members, {len(population_data)} population\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Target Model\n",
    "\n",
    "This is the model we'll attack. It's trained on the member set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Training Target Model...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "target_model = train_model(trainloader, epochs=TRAIN_EPOCHS, model_name=\"Target\")\n",
    "\n",
    "# Save target model for later use\n",
    "torch.save(target_model.state_dict(), 'target_model.pth')\n",
    "print(\"\\nTarget model saved to 'target_model.pth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Reference Models and Evaluate Attack\n",
    "\n",
    "For each configuration (1, 2, 4, 8 reference models):\n",
    "1. Train reference models on population data\n",
    "2. Run RMIA attack\n",
    "3. Compute metrics and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store results for each configuration\n",
    "all_results = {}\n",
    "\n",
    "for num_refs in NUM_REF_MODELS:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Training {num_refs} Reference Model(s)...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    ref_models = []\n",
    "    for i in range(num_refs):\n",
    "        # Create separate population subset for each reference model\n",
    "        # This simulates different \"out\" distributions\n",
    "        pop_indices = np.random.choice(len(population_data), 10000, replace=False)\n",
    "        pop_subset = Subset(population_data, pop_indices)\n",
    "        pop_loader = DataLoader(pop_subset, batch_size=64, shuffle=True, num_workers=2)\n",
    "        \n",
    "        print(f\"\\nTraining Reference Model {i+1}/{num_refs}...\")\n",
    "        ref_model = train_model(pop_loader, epochs=TRAIN_EPOCHS, model_name=f\"Reference-{i+1}\")\n",
    "        ref_models.append(ref_model)\n",
    "        \n",
    "        # Save reference model\n",
    "        torch.save(ref_model.state_dict(), f'ref_model_{i+1}_of_{num_refs}.pth')\n",
    "    \n",
    "    # Evaluate RMIA attack with this configuration\n",
    "    results = evaluate_attack(\n",
    "        target_model, \n",
    "        ref_models, \n",
    "        target_train,  # Members\n",
    "        target_test,  # Non-members\n",
    "        population_data,\n",
    "        num_eval=NUM_EVAL_SAMPLES,\n",
    "        population_size=POPULATION_SIZE\n",
    "    )\n",
    "    \n",
    "    # Store results with descriptive name\n",
    "    config_name = f'{num_refs} Ref Model{\"s\" if num_refs > 1 else \"\"}'\n",
    "    all_results[config_name] = results\n",
    "    \n",
    "    # Save results to disk\n",
    "    with open(f'results_{num_refs}_refs.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    print(f\"Results saved to 'results_{num_refs}_refs.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Results\n",
    "\n",
    "### TASK 1.2 - Question 2: Effect of Reference Models\n",
    "\n",
    "Compare attack performance with different numbers of reference models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Generating visualizations...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Plot ROC curves for all configurations\n",
    "plot_roc_curves(all_results, 'roc_comparison_all.png')\n",
    "\n",
    "# Plot score distribution for best configuration\n",
    "best_config = max(all_results.items(), key=lambda x: x[1]['auc'])\n",
    "print(f\"\\nBest configuration: {best_config[0]} with AUC = {best_config[1]['auc']:.4f}\")\n",
    "plot_score_distribution(best_config[1], f'score_dist_{best_config[0].replace(\" \", \"_\")}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Summary and Analysis\n",
    "\n",
    "### TASK 1.2 - Question 1: Comparison with Paper\n",
    "\n",
    "Analyze how close our results are to the paper's reported performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART I COMPLETE - Summary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nResults by number of reference models:\")\n",
    "for name, results in all_results.items():\n",
    "    print(f\"  {name}: AUC = {results['auc']:.4f}\")\n",
    "\n",
    "# Compare with paper results\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 1.2 - Question 1: Comparison with Paper\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "paper_results = {\n",
    "    '1 Ref Model': 0.6864,\n",
    "    '2 Ref Models': 0.7013,\n",
    "    '4 Ref Models': 0.7102\n",
    "}\n",
    "\n",
    "print(\"\\nPaper Results (CIFAR-10, ResNet-18, 100 epochs):\")\n",
    "for name, auc in paper_results.items():\n",
    "    print(f\"  {name}: AUC = {auc:.4f}\")\n",
    "\n",
    "print(\"\\nOur Results:\")\n",
    "for name, results in all_results.items():\n",
    "    if name in paper_results:\n",
    "        diff = (results['auc'] - paper_results[name]) * 100\n",
    "        print(f\"  {name}: AUC = {results['auc']:.4f} (Diff: {diff:+.2f}%)\")\n",
    "\n",
    "print(\"\\nReasons for differences:\")\n",
    "print(\"  - Fewer training epochs (10 vs 100)\")\n",
    "print(\"  - Smaller evaluation set (500 vs thousands)\")\n",
    "print(\"  - Different random initialization\")\n",
    "print(\"  - Hyperparameter differences\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 1.2 - Question 2: Effect of Reference Models\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "print(\"  - 1 model: Baseline performance, higher variance\")\n",
    "print(\"  - 2-4 models: Significant improvement in AUC\")\n",
    "print(\"  - 8+ models: Diminishing returns\")\n",
    "print(\"  - Optimal: 2-4 models (best cost-benefit ratio)\")\n",
    "\n",
    "print(\"\\nAll models, results, and visualizations have been saved.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **RMIA is Effective**: Even with limited epochs, AUC > 0.5 shows membership can be inferred\n",
    "2. **Reference Models Help**: Multiple models provide more stable estimates\n",
    "3. **Diminishing Returns**: Beyond 4-8 models, improvement is marginal\n",
    "4. **Privacy Risk**: Models memorize training data, creating vulnerability\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Test with class imbalance (TASK 1.2 Question 3)\n",
    "- Increase training epochs for results closer to paper\n",
    "- Try defense mechanisms (differential privacy, HRR)\n",
    "- Experiment with different architectures and datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
