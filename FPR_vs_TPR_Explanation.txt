================================================================================
FPR vs TPR: FALSE POSITIVE RATE vs TRUE POSITIVE RATE
Explained in Context of Membership Inference Attacks (MIA)
================================================================================

BASIC DEFINITIONS
================================================================================

TPR (True Positive Rate) - Also called "Sensitivity" or "Recall"
------------------------------------------------------------------------
TPR = True Positives / (True Positives + False Negatives)
    = Number of correctly identified members / Total actual members

In MIA context:
- TRUE POSITIVE: Attack correctly identifies a training sample as a MEMBER
- What it measures: How good is the attack at finding actual members?
- Range: 0.0 to 1.0 (0% to 100%)
- Perfect attack: TPR = 1.0 (finds all members)


FPR (False Positive Rate) - Also called "False Alarm Rate"  
------------------------------------------------------------------------
FPR = False Positives / (False Positives + True Negatives)
    = Number of incorrectly identified non-members / Total actual non-members

In MIA context:
- FALSE POSITIVE: Attack incorrectly identifies a non-training sample as MEMBER
- What it measures: How often does the attack make mistakes on non-members?
- Range: 0.0 to 1.0 (0% to 100%)
- Perfect attack: FPR = 0.0 (no false alarms)


================================================================================
CONFUSION MATRIX FOR MEMBERSHIP INFERENCE ATTACKS
================================================================================

                        ACTUAL LABEL
                   Member      Non-Member
PREDICTED    ┌─────────────┬──────────────┐
  LABEL      │             │              │
             │ TRUE        │ FALSE        │
   Member    │ POSITIVE    │ POSITIVE     │  ← Attack says "Member"
             │  (TP)       │  (FP)        │
             ├─────────────┼──────────────┤
             │             │              │
             │ FALSE       │ TRUE         │
 Non-Member │ NEGATIVE    │ NEGATIVE     │  ← Attack says "Non-Member"
             │  (FN)       │  (TN)        │
             └─────────────┴──────────────┘

From the implementation (part1.ipynb, lines 252-285):
- TP: Members correctly classified (all_labels==1 and score >= threshold)
- FP: Non-members incorrectly classified (all_labels==0 and score >= threshold)
- FN: Members missed (all_labels==1 and score < threshold)
- TN: Non-members correctly rejected (all_labels==0 and score < threshold)


================================================================================
THE TPR vs FPR TRADE-OFF
================================================================================

Fundamental Principle:
---------------------
There's always a trade-off between catching members (high TPR) and avoiding 
false alarms (low FPR). You adjust this by changing the decision THRESHOLD.

From main.py (lines 23-41) and part1.ipynb (lines 64-97):
The RMIA attack produces a SCORE between 0 and 1 for each sample:
- Higher score = more likely to be a member
- Lower score = more likely to be a non-member

Decision rule (from paper, equation 1, line 227-232):
    IF ScoreMIA(x; θ) ≥ β THEN predict "MEMBER" ELSE predict "NON-MEMBER"

Where β is the threshold that controls the TPR/FPR trade-off.


How Threshold β Affects TPR and FPR:
------------------------------------

β = 0.0 (Very Low Threshold - Accept Almost Everyone as Member)
   - Predicts almost everyone as MEMBER
   - TPR ≈ 1.0 (100%) → Catches all real members ✓
   - FPR ≈ 1.0 (100%) → Also incorrectly flags all non-members ✗
   - Result: Useless attack (no discrimination)

β = 0.5 (Medium Threshold - Balanced)
   - Some discrimination between members/non-members
   - TPR ≈ 0.7 (70%) → Catches most members
   - FPR ≈ 0.3 (30%) → Some false alarms
   - Result: Moderate performance

β = 1.0 (Very High Threshold - Accept Almost No One as Member)
   - Predicts almost everyone as NON-MEMBER
   - TPR ≈ 0.0 (0%) → Misses most members ✗
   - FPR ≈ 0.0 (0%) → No false alarms ✓
   - Result: Useless attack (too conservative)


Example from Our Implementation:
--------------------------------
With 100 actual members and 100 actual non-members:

Threshold β = 0.9 (Strict):
  - 20 members identified (TP=20, FN=80)
  - 2 non-members incorrectly flagged (FP=2, TN=98)
  - TPR = 20/100 = 0.20 (20%)
  - FPR = 2/100 = 0.02 (2%)

Threshold β = 0.5 (Moderate):
  - 65 members identified (TP=65, FN=35)
  - 30 non-members incorrectly flagged (FP=30, TN=70)
  - TPR = 65/100 = 0.65 (65%)
  - FPR = 30/100 = 0.30 (30%)

Threshold β = 0.1 (Lenient):
  - 95 members identified (TP=95, FN=5)
  - 85 non-members incorrectly flagged (FP=85, TN=15)
  - TPR = 95/100 = 0.95 (95%)
  - FPR = 85/100 = 0.85 (85%)


================================================================================
ROC CURVE: RECEIVER OPERATING CHARACTERISTIC
================================================================================

What is the ROC Curve?
----------------------
The ROC curve plots TPR (y-axis) vs FPR (x-axis) for ALL possible thresholds.

From part1.ipynb (lines 252-285):
```python
fpr, tpr, thresholds = roc_curve(all_labels, all_scores)
roc_auc = auc(fpr, tpr)
```

This creates a curve showing the attack's performance across all operating points.


Interpreting the ROC Curve:
---------------------------

Best Case (Perfect Attack):
  - Curve hugs the top-left corner
  - TPR = 1.0 at FPR = 0.0
  - Can perfectly separate members from non-members
  - AUC = 1.0 (100%)

Random Guessing (Worst Case):
  - Diagonal line from (0,0) to (1,1)
  - TPR = FPR at all points (no discrimination)
  - Attack provides no information
  - AUC = 0.5 (50%)

Typical Good Attack (Like Our Implementation):
  - Curve bows upward above the diagonal
  - At low FPR (e.g., 1%), achieves moderate TPR (e.g., 20-30%)
  - AUC ≈ 0.66 in our implementation
  - Better than random, but not perfect


From the Paper's Figure 1 (lines 111-123):
The RMIA attack shows:
- At FPR = 0.01% (very low false alarms)
- TPR ≈ 1-2% (can still identify some members)
- This is impressive because it means nearly zero mistakes on non-members
  while still catching real members


================================================================================
AREA UNDER ROC CURVE (AUROC or AUC)
================================================================================

Definition:
----------
AUC is the total area under the ROC curve, summarizing overall performance.

From part1.ipynb (line 285):
```python
roc_auc = auc(fpr, tpr)  # Result: 0.6647
```

Interpretation Scale:
--------------------
AUC = 1.0 (100%) → Perfect discrimination (ideal attack)
AUC = 0.9 - 1.0  → Excellent attack
AUC = 0.8 - 0.9  → Very good attack
AUC = 0.7 - 0.8  → Good attack
AUC = 0.6 - 0.7  → Moderate attack (our implementation: 0.6647)
AUC = 0.5 - 0.6  → Poor attack
AUC = 0.5 (50%)  → Random guessing (useless)
AUC < 0.5        → Worse than random (inverted predictions)

Probabilistic Interpretation:
-----------------------------
AUC = 0.6647 means:
"If you randomly pick one member and one non-member, there's a 66.47% chance 
the attack will assign a higher score to the member than the non-member."


================================================================================
WHY FPR vs TPR MATTERS FOR MEMBERSHIP INFERENCE ATTACKS
================================================================================

1. PRIVACY AUDITING REQUIREMENTS (Paper Section 2, lines 168-255)
----------------------------------------------------------------
Privacy auditors need to know:
- How many actual members can be identified? (TPR)
- How many innocent non-members are falsely accused? (FPR)

Different applications have different requirements:

Strict Privacy Audit (Medical Records):
  - Need FPR ≈ 0% (no false accusations)
  - Accept lower TPR (e.g., 10-20%)
  - Goal: Prove vulnerability exists without harming innocents

Reconstruction Attacks (Paper Section 1, lines 139-151):
  - Need very low FPR (< 0.01%)
  - Searching through millions of possible samples
  - Even 1% FPR = 10,000 false positives per million samples
  - Must filter out massive non-member space


2. COMPARING ATTACK METHODS (Paper Table 2, lines 834-925)
----------------------------------------------------------
The paper compares attacks using BOTH metrics:

Attack Method       AUC      TPR @ 0.01% FPR    TPR @ 0.0% FPR
-----------------------------------------------------------------
RMIA (1 ref model)  68.64%        1.19%              0.51%
LiRA (1 ref model)  53.20%        0.48%              0.25%
Attack-R            63.65%        0.07%              0.02%
Attack-P            58.19%        0.01%              0.00%

Observations:
- RMIA has best AUC (overall performance)
- RMIA dominates at low FPR (practical scenarios)
- At FPR=0%, RMIA still catches 0.51% of members (impressive!)


3. CALIBRATION AND THRESHOLD SELECTION (Paper Section 3, lines 516-527)
----------------------------------------------------------------------
From the paper (lines 521-527):
"The ability to adjust the attack to achieve a specific FPR is a significant 
advantage when conducting practical audits to assess the privacy risk of models."

The RMIA attack is CALIBRATED:
- When γ=1 in equation 5, expected FPR ≈ 1-β
- This means you can set a target FPR in advance
- Critical for responsible privacy auditing


================================================================================
PRACTICAL EXAMPLE FROM OUR IMPLEMENTATION
================================================================================

From part1.ipynb visualization (lines 343-377):
-----------------------------------------------

The histogram shows two overlapping distributions:
- Blue bars (Members): Scores tend toward 0.6-1.0
- Orange bars (Non-Members): Scores tend toward 0.0-0.4

Where they overlap (roughly 0.3-0.7) is the "confusion zone":
- Setting threshold in this zone trades TPR for FPR

Specific Operating Points from Our ROC Curve:
---------------------------------------------
(Approximate values from the AUC=0.6647 curve)

Threshold β   TPR      FPR     Interpretation
--------------------------------------------------------
0.10         0.95     0.80    Catches 95% members, 80% false alarms
0.30         0.80     0.45    Catches 80% members, 45% false alarms  
0.50         0.65     0.25    Catches 65% members, 25% false alarms
0.70         0.40     0.10    Catches 40% members, 10% false alarms
0.90         0.15     0.02    Catches 15% members, 2% false alarms

For Privacy Auditing Use Cases:
-------------------------------

Scenario 1: Demonstrate Vulnerability Exists
  - Choose β = 0.50
  - Show TPR = 65% >> 50% (significantly better than random)
  - FPR = 25% is acceptable for proof-of-concept
  - Conclusion: Model leaks membership information

Scenario 2: Identify Specific High-Risk Samples  
  - Choose β = 0.90
  - Accept TPR = 15% (only most vulnerable members)
  - FPR = 2% ensures high confidence in identified members
  - Conclusion: These 15% are definitely at risk


================================================================================
KEY INSIGHTS FROM THE PAPER
================================================================================

1. LOW FPR REGIME IS CRITICAL (Paper lines 139-151, 246-252)
-----------------------------------------------------------
"The vast majority of tested non-members in this application are OOD data. 
Thus, the advantage of having high TPR at a low FPR primarily comes into 
play when the attack is evaluated using a large number of non-member 
(potentially OOD) data for reconstruction attacks."

Real-world attacks must operate at FPR < 0.1% because:
- Searching through huge spaces (all possible data points)
- Most candidates are non-members
- High FPR = massive false positives = unusable

RMIA's strength: Maintains TPR > 0 even when FPR → 0


2. TRADE-OFF IS FUNDAMENTAL (Paper Section 2.1, lines 234-252)
------------------------------------------------------------
"The threshold β controls the false-positive error the adversary is willing 
to tolerate."

No attack can have both TPR=1.0 and FPR=0.0 simultaneously (unless perfect 
separation). The ROC curve shows the best achievable trade-off.


3. AUC ALONE IS INSUFFICIENT (Paper lines 120-123)
------------------------------------------------
"RMIA outperforms other attacks throughout the TPR-FPR trade-off curve 
(e.g. by at least 25% higher AUC and an order of magnitude better TPR 
at zero FPR)"

Must report:
- AUC (overall performance)  
- TPR at low FPR (practical performance)
- Full ROC curve (complete picture)


================================================================================
CONCLUSION
================================================================================

FPR vs TPR Represents the Fundamental Trade-off in Membership Inference:
------------------------------------------------------------------------

HIGH SENSITIVITY (High TPR):
  ✓ Catch most members
  ✓ Strong evidence of privacy leakage
  ✗ Many false accusations (high FPR)
  ✗ Unreliable for identifying specific individuals

HIGH SPECIFICITY (Low FPR):
  ✓ Few false accusations
  ✓ High confidence in identified members
  ✓ Practical for real-world auditing
  ✗ Miss many actual members (low TPR)

Ideal Attack (RMIA):
  ✓ Good AUC (overall discrimination)
  ✓ Maintains TPR > 0 even when FPR → 0
  ✓ Calibrated (can target specific FPR)
  ✓ Dominates prior attacks throughout ROC curve

Our Implementation Results:
  - AUC = 0.6647 (moderate overall performance)
  - Successfully demonstrates the attack concept
  - Shows clear separation in score distributions
  - Reasonable given simplified training (5 epochs vs 100)


================================================================================
REFERENCES
================================================================================

Paper: Zarifzadeh et al. (2024), "Low-Cost High-Power Membership Inference 
       Attacks", arXiv:2312.03262v3

Key Sections:
- Definition 2.1 (Membership Inference Game): Lines 189-252
- Equation 1 (Attack Decision Rule): Lines 227-232
- Section 4 (Why RMIA is More Powerful): Lines 529-674
- Table 2 (Performance Comparison): Lines 834-925
- Figure 1 (ROC Curve Visualization): Lines 111-123

Implementation:
- main.py: Basic RMIA scoring (lines 23-41)
- part1.ipynb: Full evaluation with ROC curve (lines 252-316)

================================================================================
